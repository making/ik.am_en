---
title: Tracing Tanzu Application Platform's Contour with OpenTelemetry
tags: ["Kubernetes", "Tanzu", "TAP", "Grafana", "Tempo", "Tracing", "Contour", "OpenTelemetry"]
categories: ["Dev", "CaaS", "Kubernetes", "TAP", "Tracing"]
date: 2024-01-26T08:17:54Z
updated: 2024-01-26T23:11:25Z
---

Contour, used as the Ingress Controller for Tanzu Application Platform, has supported [Tracing](https://projectcontour.io/docs/1.27/config/tracing/) from version 1.25.
Since TAP 1.7 uses Contour 1.25, this feature is available.

Contour only supports Tracing via OpenTelemetry (OTLP). In this case, we use [Tempo](https://grafana.com/oss/tempo/) as the Tracing backend that supports the OTLP protocol. Additionally, we use [Grafana](https://grafana.com/oss/grafana/) as the UI for Tempo.

First, we create the following configuration.

<img width="910" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/4f102edf-2e5c-485b-a39d-d77288c3a18c">

**Table of Contents**
<!-- toc -->

### Installing Tempo

Install Tempo with helm.

```
helm repo add grafana https://grafana.github.io/helm-charts
```

```
helm upgrade tempo \
  -n tempo \
  grafana/tempo \
  --set tempo.receivers.zipkin=null \
  --create-namespace \
  --install \
  --wait
```

Check the Pod.

```
$ kubectl get pod -n tempo
NAME      READY   STATUS    RESTARTS   AGE
tempo-0   1/1     Running   0          9s
```

### Installing Grafana

Since Tempo does not have a UI, we install Grafana as the UI. Grafana is also installed with helm.

Please change the domain name and Cluster Issuer name according to your environment.

```yaml
cat <<EOF > helm-values.yaml
---
adminUser: grafana
adminPassword: grafana
testFramework:
  enabled: false
ingress:
  enabled: true
  hosts:
  - grafana.tapv-huge-hornet.tapsandbox.com
  tls:
  - hosts:
    - grafana.tapv-huge-hornet.tapsandbox.com
    secretName: grafana-tls
  annotations:
    cert-manager.io/cluster-issuer: tap-ingress-selfsigned
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Tempo
      uid: grafana-traces
      type: tempo
      access: proxy
      orgId: 1
      url: http://tempo.tempo.svc.cluster.local:3100
      editable: true
---
EOF
```

```
helm upgrade grafana \
  -n grafana \
  grafana/grafana \
  -f helm-values.yaml \
  --create-namespace \
  --install \
  --wait
```

Check the pod and ingress.

```
$ kubectl get pod,ing -n grafana
NAME                           READY   STATUS    RESTARTS   AGE
pod/grafana-7cb85d6cfc-qp2j8   1/1     Running   0          2m36s

NAME                                CLASS    HOSTS                                     ADDRESS         PORTS     AGE
ingress.networking.k8s.io/grafana   <none>   grafana.tapv-huge-hornet.tapsandbox.com   35.238.162.93   80, 443   2m36s
```

Access the URL of Grafana. The username and password are both `grafana`.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/fdfe8065-8d65-420f-af13-7a86387ce8c7">

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/34de8239-9dd5-4957-bb26-2bb93ba8158c">

Select the Tempo data source in Explore. There is no data yet.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/cf3848bd-7819-4d3e-8f3a-4536e3f8760b">

### Tracing with Contour

Following [Contour's documentation](https://projectcontour.io/docs/1.27/config/tracing/), create the following ExtensionService resource.

```yaml
cat <<EOF > tempo-extension.yaml
---
apiVersion: projectcontour.io/v1alpha1
kind: ExtensionService
metadata:
  name: tempo
  namespace: tempo
spec:
  protocol: h2c
  services:
  - name: tempo
    port: 4317
---
EOF

kubectl apply -f tempo-extension.yaml
```

Add the following settings to `tap-values.yaml` to include tracing settings in Contour's config file. Here, we change Envoy's access log to JSON format and ensure it includes the `traceparent` field.

```yaml
# ...
contour:
  contour:
    configFileContents:
      tracing:
        includePodDetail: true
        extensionService: tempo/tempo
        serviceName: contour
      accesslog-format: json
      json-fields:
      - "@timestamp"
      - "authority"
      - "bytes_received"
      - "bytes_sent"
      - "traceparent=%REQ(TRACEPARENT)%" # <--
      - "duration"
      - "method"
      - "path"
      - "protocol"
      - "referer=%REQ(REFERER)%"
      - "request_id"
      - "requested_server_name"
      - "response_code"
      - "upstream_cluster"
      - "user_agent"
      - "x_forwarded_for"
# ...
```

Update TAP.

```
tanzu package installed update tap -n tap-install --values-file tap-values.yaml 
```

It seems that the settings are not applied unless Contour is explicitly restarted.

```
kubectl delete pod -n tanzu-system-ingress -l app=contour --force
```

Use the following command to check Envoy's access logs.

```
kubectl logs -n tanzu-system-ingress -l app=envoy -c envoy -f
```

When accessing an app on TAP, you can see JSON logs like the following.

```
{"referer":null,"duration":38,"upstream_cluster":"apps_rest-service-00004_80","user_agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36","bytes_sent":60,"protocol":"HTTP/2","x_forwarded_for":"192.168.3.1","requested_server_name":"rest-service-apps.tapv-huge-hornet.tapsandbox.com","path":"/greeting","traceparent":"00-7a352f8ac8b545bce79e439cbe595687-dbd8a641798bfa4d-01","@timestamp":"2024-01-25T09:34:43.018Z","method":"GET","request_id":"aeff3400-fed7-9e1f-bf39-de29ee7b7078","authority":"rest-service-apps.tapv-huge-hornet.tapsandbox.com","bytes_received":0,"response_code":200}
```

The `traceparent` field is set to `00-7a352f8ac8b545bce79e439cbe595687-dbd8a641798bfa4d-01`, which is in the [W3C Tracing Context](https://www.w3.org/TR/trace-context/) format.
`7a352f8ac8b545bce79e439cbe595687` is the Trace ID.

Search for the Trace in Grafana's Explore using the Trace ID. You can see the Trace equivalent to the access log measured at the Contour level.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6d198a32-3a2b-4863-b379-9b978af4e974">

You can also retrieve a list of Traces.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/83ef2250-9fa2-425d-8fc6-0064f72c1f3b">

### Sending Traces from the App

This time, we propagate the Trace Context to the app and perform Tracing on the app side, sending the Trace to Tempo.

<img width="802" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/a6cbd19e-e166-41e0-9f62-da61308faeaa">

We use https://github.com/categolj/blog-api for the app. This app performs Tracing with [Micrometer Tracing](https://docs.micrometer.io/tracing/reference/).
At the time of writing this article, the app uses the Zipkin protocol for Tracing, so we set the environment variable `MANAGEMENT_ZIPKIN_TRACING_ENDPOINT` to send Traces to Tempo's 9411 port.

Deploy the app with the following command. PostgreSQL is required, so we create a PostgreSQL instance with Bitnami Service.

```
tanzu service class-claim create blog-db --class postgresql-unmanaged --parameter storageGB=1 -n demo

tanzu apps workload apply blog-api \
  --app blog-api \
  --git-repo https://github.com/categolj/blog-api \
  --git-branch main \
  --type web \
  --annotation autoscaling.knative.dev/minScale=1 \
  --label apps.tanzu.vmware.com/has-tests=true \
  --service-ref blog-db=services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:blog-db \
  --build-env BP_JVM_VERSION=17 \
  --env MANAGEMENT_ZIPKIN_TRACING_ENDPOINT=http://tempo.tempo.svc.cluster.local:9411 \
  -n apps
```

Send requests to the app.

```
curl -s https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/template.md > template.md
curl -s -u admin:changeme -XPUT https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/2 -H "Content-Type: text/markdown" -d "$(cat template.md)"
curl -s https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/2 | jq .
```

The following logs are output in Envoy's access logs.

```
{"@timestamp":"2024-01-25T10:06:37.820Z","authority":"blog-api-apps.tapv-huge-hornet.tapsandbox.com","bytes_received":197,"requested_server_name":"blog-api-apps.tapv-huge-hornet.tapsandbox.com","path":"/entries/2","user_agent":"curl/8.1.2","upstream_cluster":"apps_blog-api-00002_80","response_code":200,"traceparent":"00-98a0891be5a08d923da6feebb2aab04f-8ef8b9f68a8ee1b2-01","bytes_sent":412,"x_forwarded_for":"10.0.1.8","method":"PUT","referer":null,"duration":967,"request_id":"2c9cc4c5-e2c7-90f6-8680-8eeab7fbdaf2","protocol":"HTTP/2"}
```

The Trace ID for this request is `98a0891be5a08d923da6feebb2aab04f`.

Viewing this Trace's information in Grafana, we can see that it includes Spans from not only Contour but also the app side.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/2d6f3d7f-3c2e-4d3c-a940-ffebf365b6ef">

Looking at the details of the Span, we can also see the executed SQL.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/e6b8e51f-396c-472f-a82c-2f13e1575023">

By enabling Tracing in Contour, it has become easy to view the Trace data of the app starting from Envoy's access logs.

By the way, when viewing the list of Trace data in Tempo in Grafana, you will see many Traces (Traces for access to Grafana itself).
Since Tracing is performed at the Contour level, all requests passing through Envoy are targeted. Depending on the case, this can become Noisy data, so you may want to filter it.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6a53b7ac-967f-4a04-9023-4354901198eb">

Filtering Spans is probably not possible in Tempo, or even if it is possible, it would become Tempo-specific know-how.
Here, with the idea of replacing the Trace backend in mind, we insert a vendor-neutral [Open Telemetry Collector](https://github.com/open-telemetry/opentelemetry-collector) in between and filter Spans at the Collector level.

### Introducing Open Telemetry Collector

We introduce Open Telemetry Collector and change the destination of Trace transmission from Envoy to Open Telemetry Collector. Open Telemetry Collector filters Spans and then sends the data to Tempo. The Trace transmission destination of the app can also be changed to Open Telemetry Collector, but here we only change the configuration of Contour.

<img width="1017" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6135fcbd-8f67-446e-9e9c-080daef65aac">

We use [Open Telemetry Operator](https://github.com/open-telemetry/opentelemetry-operator) to install Open Telemetry Collector on Kubernetes.

Install Open Telemetry Operator with the following command.

```
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
```

Here, we used Open Telemetry Operator 0.92.1. To install a specific version, use the following command.

```
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.92.1/opentelemetry-operator.yaml
```

Check the Pod.

```
$ kubectl get pod -n opentelemetry-operator-system
NAME                                                         READY   STATUS    RESTARTS   AGE
opentelemetry-operator-controller-manager-5fb8cbf79b-8xlkl   2/2     Running   0          104m
```

Using Open Telemetry Operator, you can install Open Telemetry Collector by using the OpenTelemetryCollector resource.

Create the OpenTelemetryCollector resource as follows. We defined a Trace pipeline that accepts OTLP, filters Spans, and then sends data to Tempo via OTLP.

```yaml
cat <<'EOF' > otelcol.yaml
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: opentelemetry
spec:
  config: |
    receivers:
      otlp:
        protocols:
          grpc: {}
          http: {}
    processors:
      filter:
        # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor
        error_mode: ignore
        traces:
          span:
          - IsMatch(attributes["upstream_cluster"], "grafana/.*")
          - IsMatch(attributes["upstream_cluster"], "tap-gui/.*")
          - IsMatch(attributes["upstream_cluster"], "appsso/.*")
          - IsMatch(attributes["http.url"], "https://grafana.*")
      batch:
        send_batch_size: 10000
        timeout: 10s
    exporters: 
      otlp/tempo:
        endpoint: http://tempo.tempo.svc.cluster.local:4317
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          receivers:
          - otlp
          processors:
          - filter
          - batch
          exporters:
          - otlp/tempo
---
EOF

kubectl create namespace opentelemetry
kubectl apply -f otelcol.yaml
```

Check the OpenTelemetryCollector, Pod, and Service resources.

```
$ kubectl get otelcol,pod,svc -n opentelemetry
NAME                                           MODE         VERSION   READY   AGE   IMAGE                                                                                    MANAGEMENT
opentelemetrycollector.opentelemetry.io/otel   deployment   0.92.0    1/1     6s    ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.92.0   managed

NAME                                  READY   STATUS    RESTARTS   AGE
pod/otel-collector-75fb5c8dc5-kssmp   1/1     Running   0          5s

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/otel-collector              ClusterIP   192.168.73.127   <none>        4317/TCP,4318/TCP   7s
service/otel-collector-headless     ClusterIP   None             <none>        4317/TCP,4318/TCP   7s
service/otel-collector-monitoring   ClusterIP   192.168.68.138   <none>        8888/TCP            7s
```

Change Contour's Trace settings from Tempo to Open Telemetry Collector.

```yaml
cat <<EOF > otelcol-extension.yaml
---
apiVersion: projectcontour.io/v1alpha1
kind: ExtensionService
metadata:
  name: otel-collector
  namespace: opentelemetry
spec:
  protocol: h2c
  services:
  - name: otel-collector
    port: 4317
---
EOF

kubectl apply -f otelcol-extension.yaml
```

Also, update `tap-values.yaml` accordingly.

```yaml
contour:
  contour:
    configFileContents:
      tracing:
        includePodDetail: true
        extensionService: opentelemetry/otel-collector #! <---
        serviceName: contour
```

Update TAP.

```
tanzu package installed update tap -n tap-install --values-file tap-values.yaml 
```

Restart Contour.

```
kubectl delete pod -n tanzu-system-ingress -l app=cont
