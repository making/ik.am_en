---
title: Notes on Tracing Contour in Tanzu Application Platform with OpenTelemetry
tags: ["Kubernetes", "Tanzu", "TAP", "Grafana", "Tempo", "Tracing", "Contour", "OpenTelemetry"]
categories: ["Dev", "CaaS", "Kubernetes", "TAP", "Tracing"]
date: 2024-01-26T08:17:54Z
updated: 2024-01-26T23:11:25Z
---

> ⚠️ This article was automatically translated by OpenAI (gpt-4o).
> It may be edited eventually, but please be aware that it may contain incorrect information at this time.

Tanzu Application Platform's Ingress Controller, Contour, has supported [Tracing](https://projectcontour.io/docs/1.27/config/tracing/) since version 1.25. TAP 1.7 uses Contour 1.25, so this feature is available.

Contour only supports tracing via OpenTelemetry (OTLP). This time, we will use [Tempo](https://grafana.com/oss/tempo/) as the tracing backend that supports the OTLP protocol. We will also use [Grafana](https://grafana.com/oss/grafana/) as the UI for Tempo.

First, let's create the following configuration.

<img width="910" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/4f102edf-2e5c-485b-a39d-d77288c3a18c">

**Table of Contents**
<!-- toc -->

### Installing Tempo

Install Tempo using helm.

```
helm repo add grafana https://grafana.github.io/helm-charts
```

```
helm upgrade tempo \
  -n tempo \
  grafana/tempo \
  --set tempo.receivers.zipkin=null \
  --create-namespace \
  --install \
  --wait
```

Check the Pod.

```
$ kubectl get pod -n tempo
NAME      READY   STATUS    RESTARTS   AGE
tempo-0   1/1     Running   0          9s
```

### Installing Grafana

Since Tempo does not have a UI, install Grafana as the UI. Grafana is also installed using helm.

Change the domain name and Cluster Issuer name according to your environment.

```yaml
cat <<EOF > helm-values.yaml
---
adminUser: grafana
adminPassword: grafana
testFramework:
  enabled: false
ingress:
  enabled: true
  hosts:
  - grafana.tapv-huge-hornet.tapsandbox.com
  tls:
  - hosts:
    - grafana.tapv-huge-hornet.tapsandbox.com
    secretName: grafana-tls
  annotations:
    cert-manager.io/cluster-issuer: tap-ingress-selfsigned
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Tempo
      uid: grafana-traces
      type: tempo
      access: proxy
      orgId: 1
      url: http://tempo.tempo.svc.cluster.local:3100
      editable: true
---
EOF
```

```
helm upgrade grafana \
  -n grafana \
  grafana/grafana \
  -f helm-values.yaml \
  --create-namespace \
  --install \
  --wait
```

Check the pod and ingress.

```
$ kubectl get pod,ing -n grafana
NAME                           READY   STATUS    RESTARTS   AGE
pod/grafana-7cb85d6cfc-qp2j8   1/1     Running   0          2m36s

NAME                                CLASS    HOSTS                                     ADDRESS         PORTS     AGE
ingress.networking.k8s.io/grafana   <none>   grafana.tapv-huge-hornet.tapsandbox.com   35.238.162.93   80, 443   2m36s
```

Access the Grafana URL. Both the username and password are `grafana`.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/fdfe8065-8d65-420f-af13-7a86387ce8c7">

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/34de8239-9dd5-4957-bb26-2bb93ba8158c">

Select the Tempo data source in Explore. There is no data yet.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/cf3848bd-7819-4d3e-8f3a-4536e3f8760b">

### Tracing with Contour

Following the [Contour documentation](https://projectcontour.io/docs/1.27/config/tracing/), create the following ExtensionService resource.

```yaml
cat <<EOF > tempo-extension.yaml
---
apiVersion: projectcontour.io/v1alpha1
kind: ExtensionService
metadata:
  name: tempo
  namespace: tempo
spec:
  protocol: h2c
  services:
  - name: tempo
    port: 4317
---
EOF

kubectl apply -f tempo-extension.yaml
```

Add the following settings to `tap-values.yaml` to include tracing settings in the Contour config file. Here, we change the Envoy access log to JSON format and include the `traceparent` field.

```yaml
# ...
contour:
  contour:
    configFileContents:
      tracing:
        includePodDetail: true
        extensionService: tempo/tempo
        serviceName: contour
      accesslog-format: json
      json-fields:
      - "@timestamp"
      - "authority"
      - "bytes_received"
      - "bytes_sent"
      - "traceparent=%REQ(TRACEPARENT)%" # <--
      - "duration"
      - "method"
      - "path"
      - "protocol"
      - "referer=%REQ(REFERER)%"
      - "request_id"
      - "requested_server_name"
      - "response_code"
      - "upstream_cluster"
      - "user_agent"
      - "x_forwarded_for"
# ...
```

Update TAP.

```
tanzu package installed update tap -n tap-install --values-file tap-values.yaml 
```

It seems that this setting is not reflected unless Contour is explicitly restarted.

```
kubectl delete pod -n tanzu-system-ingress -l app=contour --force
```

Check the Envoy access log with the following command.

```
kubectl logs -n tanzu-system-ingress -l app=envoy -c envoy -f
```

When you access an app on TAP, you can see the following JSON log.

```
{"referer":null,"duration":38,"upstream_cluster":"apps_rest-service-00004_80","user_agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36","bytes_sent":60,"protocol":"HTTP/2","x_forwarded_for":"192.168.3.1","requested_server_name":"rest-service-apps.tapv-huge-hornet.tapsandbox.com","path":"/greeting","traceparent":"00-7a352f8ac8b545bce79e439cbe595687-dbd8a641798bfa4d-01","@timestamp":"2024-01-25T09:34:43.018Z","method":"GET","request_id":"aeff3400-fed7-9e1f-bf39-de29ee7b7078","authority":"rest-service-apps.tapv-huge-hornet.tapsandbox.com","bytes_received":0,"response_code":200}
```

You can see that the `traceparent` field is set to `00-7a352f8ac8b545bce79e439cbe595687-dbd8a641798bfa4d-01`. This is in the [W3C Tracing Context](https://www.w3.org/TR/trace-context/) format.
`7a352f8ac8b545bce79e439cbe595687` is the Trace ID.

Search for the Trace by Trace ID from Grafana's Explore. You can see the Trace equivalent to the access log measured at the Contour level.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6d198a32-3a2b-4863-b379-9b978af4e974">

You can also get a list of Traces.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/83ef2250-9fa2-425d-8fc6-0064f72c1f3b">

### Sending Traces from the App

Next, propagate the Trace Context to the app, perform tracing on the app side, and send the Trace to Tempo.

<img width="802" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/a6cbd19e-e166-41e0-9f62-da61308faeaa">

We will use https://github.com/categolj/blog-api for the app. This app performs tracing with [Micrometer Tracing](https://docs.micrometer.io/tracing/reference/).
At the time of writing, this app uses Zipkin as the tracing protocol, so set the environment variable `MANAGEMENT_ZIPKIN_TRACING_ENDPOINT` to send traces to Tempo's 9411 port.

Deploy the app with the following command. Since PostgreSQL is required, create a PostgreSQL instance with Bitnami Service.

```
tanzu service class-claim create blog-db --class postgresql-unmanaged --parameter storageGB=1 -n demo

tanzu apps workload apply blog-api \
  --app blog-api \
  --git-repo https://github.com/categolj/blog-api \
  --git-branch main \
  --type web \
  --annotation autoscaling.knative.dev/minScale=1 \
  --label apps.tanzu.vmware.com/has-tests=true \
  --service-ref blog-db=services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:blog-db \
  --build-env BP_JVM_VERSION=17 \
  --env MANAGEMENT_ZIPKIN_TRACING_ENDPOINT=http://tempo.tempo.svc.cluster.local:9411 \
  -n apps
```

Send a request to the app.

```
curl -s https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/template.md > template.md
curl -s -u admin:changeme -XPUT https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/2 -H "Content-Type: text/markdown" -d "$(cat template.md)"
curl -s https://blog-api-apps.tapv-huge-hornet.tapsandbox.com/entries/2 | jq .
```

The following log is output in the Envoy access log.

```
{"@timestamp":"2024-01-25T10:06:37.820Z","authority":"blog-api-apps.tapv-huge-hornet.tapsandbox.com","bytes_received":197,"requested_server_name":"blog-api-apps.tapv-huge-hornet.tapsandbox.com","path":"/entries/2","user_agent":"curl/8.1.2","upstream_cluster":"apps_blog-api-00002_80","response_code":200,"traceparent":"00-98a0891be5a08d923da6feebb2aab04f-8ef8b9f68a8ee1b2-01","bytes_sent":412,"x_forwarded_for":"10.0.1.8","method":"PUT","referer":null,"duration":967,"request_id":"2c9cc4c5-e2c7-90f6-8680-8eeab7fbdaf2","protocol":"HTTP/2"}
```

You can see that the Trace ID for this request is `98a0891be5a08d923da6feebb2aab04f`.

Check the information of this Trace in Grafana. This time, you can see that it includes not only the Contour level but also the spans from the app side.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/2d6f3d7f-3c2e-4d3c-a940-ffebf365b6ef">

When you look at the details of the span, you can also see the SQL being executed.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/e6b8e51f-396c-472f-a82c-2f13e1575023">

By enabling tracing in Contour, you can easily see the trace data of the app starting from the Envoy access log.

By the way, when looking at the list of Tempo trace data in Grafana, you will see a lot of traces like the following (traces for access to Grafana itself).
Since tracing is done at the Contour level, all requests that go through Envoy are subject to tracing. In some cases, this can result in noisy data, so we want to filter this out.

<img width="1912" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6a53b7ac-967f-4a04-9023-4354901198eb">

Filtering spans is probably not possible with Tempo, or even if it is, it would become specific to Tempo. Here, we will insert a vendor-neutral [Open Telemetry Collector](https://github.com/open-telemetry/opentelemetry-collector) in between, with the idea of replacing the trace backend, and filter spans at the Collector level.

### Introducing Open Telemetry Collector

Introduce Open Telemetry Collector and change the trace destination from Envoy to Open Telemetry Collector. Open Telemetry Collector will filter spans and then send the data to Tempo. The trace destination of the app can also be changed to Open Telemetry Collector, but here we will only change the Contour settings.

<img width="1017" alt="image" src="https://github.com/making/blog.ik.am/assets/106908/6135fcbd-8f67-446e-9e9c-080daef65aac">

To install Open Telemetry Collector on Kubernetes, use the [Open Telemetry Operator](https://github.com/open-telemetry/opentelemetry-operator).

Install Open Telemetry Operator with the following command.

```
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
```

Here, we used Open Telemetry Operator 0.92.1. To specify a particular version, use the following command.

```
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.92.1/opentelemetry-operator.yaml
```

Check the Pod.

```
$ kubectl get pod -n opentelemetry-operator-system
NAME                                                         READY   STATUS    RESTARTS   AGE
opentelemetry-operator-controller-manager-5fb8cbf79b-8xlkl   2/2     Running   0          104m
```

Using Open Telemetry Operator, you can install Open Telemetry Collector using the OpenTelemetryCollector resource.

Create the OpenTelemetryCollector resource as follows. We defined a trace pipeline that receives via OTLP, filters spans, and then sends data to Tempo via OTLP.

```yaml
cat <<'EOF' > otelcol.yaml
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: opentelemetry
spec:
  config: |
    receivers:
      otlp:
        protocols:
          grpc: {}
          http: {}
    processors:
      filter:
        # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor
        error_mode: ignore
        traces:
          span:
          - IsMatch(attributes["upstream_cluster"], "grafana/.*")
          - IsMatch(attributes["upstream_cluster"], "tap-gui/.*")
          - IsMatch(attributes["upstream_cluster"], "appsso/.*")
          - IsMatch(attributes["http.url"], "https://grafana.*")
      batch:
        send_batch_size: 10000
        timeout: 10s
    exporters: 
      otlp/tempo:
        endpoint: http://tempo.tempo.svc.cluster.local:4317
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          receivers:
          - otlp
          processors:
          - filter
          - batch
          exporters:
          - otlp/tempo
---
EOF

kubectl create namespace opentelemetry
kubectl apply -f otelcol.yaml
```

Check the OpenTelemetryCollector, Pod, and Service resources.

```
$ kubectl get otelcol,pod,svc -n opentelemetry
NAME                                           MODE         VERSION   READY   AGE   IMAGE                                                                                    MANAGEMENT
opentelemetrycollector.opentelemetry.io/otel   deployment   0.92.0    1/1     6s    ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.92.0   managed

NAME                                  READY   STATUS    RESTARTS   AGE
pod/otel-collector-75fb5c8dc5-kssmp   1/1     Running   0          5s

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/otel-collector              ClusterIP   192.168.73.127   <none>        4317/TCP,4318/TCP   7s
service/otel-collector-headless     ClusterIP   None             <none>        4317/TCP,4318/TCP   7s
service/otel-collector-monitoring   ClusterIP   192.168.68.138   <none>        8888/TCP            7s
```

Change the Contour trace settings from Tempo to Open Telemetry Collector.

```yaml
cat <<EOF > otelcol-extension.yaml
---
apiVersion: projectcontour.io/v1alpha1
kind: ExtensionService
metadata:
  name: otel-collector
  namespace: opentelemetry
spec:
  protocol: h2c
  services:
  - name: otel-collector
    port: 4317
---
EOF

kubectl apply -f otelcol-extension.yaml
```

Also change `tap-values.yaml`.

```yaml
contour:
  contour:
    configFileContents:
      tracing:
        includePodDetail: true
        extensionService: opentelemetry/otel-collector #! <---
        serviceName: contour
```

Update TAP.

```
tanzu package installed update tap -n tap-install --values-file tap-values.yaml 
```

Restart Contour.

```
kubectl delete pod -n tanzu-system-ingress -
