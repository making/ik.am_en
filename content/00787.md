---
title: Using llama-cpp-python to Launch an OpenAI-Compliant Server with the Gemma Model and Accessing it via Spring AI
tags: ["Python", "llama.cpp", "OpenAI", "Machine Learning", "MPS", "Gemma", "Spring AI"]
categories: ["AI", "LLM", "llama.cpp"]
date: 2024-02-25T07:34:19Z
updated: 2024-05-10T09:18:21Z
---

> ⚠️ This article was automatically translated by OpenAI (deepseek-r1-32b).
> It may be edited eventually, but please be aware that it may contain incorrect information at this time.

"[Using llama-cpp-python to Set Up Text Generation and OpenAI-Compliant Server with Local LLM](/entries/770)" similarly, let's try Google's [Gemma](https://huggingface.co/google/gemma-2b).

**Table of Contents**
<!-- toc -->

### Installation of llama-cpp-python

First, create a virtual environment.

```bash
mkdir -p $HOME/work/llm
cd $HOME/work/llm
python3 -m venv .venv
source .venv/bin/activate
```

Install llama-cpp-python along with the server component.

```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install --force-reinstall --no-cache-dir 'llama-cpp-python[server]'
```

> ℹ️ If encountering errors on Apple Silicon Macs, try the setup instructions at [GitHub](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md).

The `chat_format="gemma"` option was supported in commit [251a8a2](https://github.com/abetlen/llama-cpp-python/commit/251a8a2cadb4c0df4671062144d168a7874086a2), so use v0.2.48 or newer.

### Downloading Gemma

```bash
sudo mkdir -p /opt/models
sudo chown -R $USER /opt/models
```

Since the 7B model is large, download the 2B version.

[Download from Hugging Face](https://huggingface.co/google/gemma-2b/tree/main)

Place `gemma-2b.gguf` in `/opt/models/`.

### Starting the OpenAI-Compliant Server

Launch the server with the following command. The `--chat_format=gemma` flag is required.

```bash
python3 -m llama_cpp.server --chat_format=gemma --model /opt/models/gemma-2b-it.gguf --n_gpu_layers 1
```

API documentation is available at:

[http://localhost:8000/docs](http://localhost:8000/docs)

> OpenAI's ["Create chat completion" API](https://platform.openai.com/docs/api-reference/chat/create) requires a `model` parameter, but llama-cpp-python seems to work without it.

#### Accessing via curl

```bash
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
   "messages": [
      {"role": "user", "content": "Give me a joke."}
   ]
 }' | jq .
```

A joke was returned:

```json
{
  "id": "chatcmpl-79f5ae4c-cf47-494c-a82c-a7e3747ab463",
  "object": "chat.completion",
  "created": 1708846379,
  "model": "/opt/models/gemma-2b-it.gguf",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!",
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 18,
    "total_tokens": 32
  }
}
```

```bash
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
   "messages": [
      {"role": "user", "content": "日本の首都はどこですか？"}
   ]
 }' | jq .
```

Japanese responses also work:

```json
{
  "id": "chatcmpl-3f111b5e-4244-4cfc-9818-d23b8d04ccb2",
  "object": "chat.completion",
  "created": 1708846400,
  "model": "/opt/models/gemma-2b-it.gguf",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "日本の首都は東京です。東京は日本の東部に位置し、日本を代表する都市です。",
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 22,
    "total_tokens": 36
  }
}
```

#### Accessing via Spring AI

Access the server from a Spring AI application. Since it's OpenAI-compliant, use Spring AI's [OpenAI Chat Client](https://docs.spring.io/spring-ai/reference/api/clients/openai-chat.html).

Sample application: [GitHub](https://github.com/making/hello-spring-ai)

```bash
git clone https://github.com/making/hello-spring-ai
cd hello-spring-ai
./mvnw clean package -DskipTests=true
java -jar target/hello-spring-ai-0.0.1-SNAPSHOT.jar --spring.ai.openai.base-url=http://localhost:8000 --spring.ai.openai.api-key=dummy
```

```bash
$ curl localhost:8080
What do you call a boomerang that won't come back?

A stick.
```

This app is designed for OpenAI but can be adapted for Gemma by changing properties. Alternatively, use [spring-ai-ollama](https://docs.spring.io/spring-ai/reference/api/clients/ollama-chat.html) to access [Ollama](https://ollama.com/) directly.
